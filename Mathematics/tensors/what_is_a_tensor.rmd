---
title: "What is a Tensor?
"
---

\tableofcontents
\clearpage

# Elementary vector spaces

Vector space is a set of vectors. However, it is important to talk about specific requirements that this mathematical set has to satisfy. 

Properties of vector space $\mathcal{W}$:

1. **Vector addition**. One should be able to add vectors, but only those that are in the vector space. If $\matr{w}, \matr{s} \in \mathcal{W}$, then $\matr{w} + \matr{s} = \matr{t} \in \mathcal{W}$.
2. **Scalar multiplication**. If $a \in \mathbb{R}$ and $\matr{w} \in \mathcal{W}$, then $a\matr{w} \in \mathcal{W}$. Vector spaces can be either real or complex, depending on what type of vector $a$ is.
3. **Linearity**:
   * $a\matr{w} + b\matr{v} = \matr{z}$
   * $a\matr{w} + a\matr{t} = a(\matr{w} + \matr{t})$
4. **Additive inverses**. If $\matr{w} \in \mathcal{W}$, then there exists another vector $-\matr{w} \in \mathcal{W}$, so that $\matr{w} + (-\matr{w}) = \matr{0}$.

Suppose we have $\matr{w}, \matr{q} \in \mathcal{V}$ and $\matr{r}, \matr{s} \in \mathcal{W}$. We can add $\matr{w}$ and $\matr{q}$ together or $\matr{r}$ and $\matr{s}$ together but not $\matr{w}$ and $\matr{r}$ together. Addition represents different operations for different vector spaces. Or at least there is no definition of how to add vectors that are not part of a particular vector space.

It is often not possible to distinguish between two vector spaces. But one property that often helps to distinguish them is their dimension. If $n$ is the *minimum* number of vectors that can be used to distinguish *any* vector in vector space $\mathcal{V}$ as their linear combination, then the dimension of $\mathcal{V}$ is $n$, i.e. $\dim{\mathcal{V}} = n$. These $n$ vectors are usually referred to as **basis vectors**. Basis vectors are not unique because if $\matr{v}$ is a basis vector, then so is $a\matr{v}$.

If two vector spaces $\mathcal{V}$ and $\mathcal{W}$ are real vector spaces and both have dimension $n$, then the difference between them is superficial---only their names are different. We call such vector spaces **isomorphic**. More precisely, two vector spaces are isomorphic if there is a one-to-one correspondence between them and if operations in one vector space are in correspondence to operations in another vector space.

In elementary vector spaces, concepts, such as dot product of vectors, cross product of vectors or magnitude of vectors, do not exist. These properties are advanced and have to be added to elementary vector spaces.

# How to make a map

Suppose we have a real vector space $\mathcal{V}$ with $\dim{\mathcal{V}} = 4$. Suppose also that we can have basis vectors $\matr{w}, \matr{v}, \matr{p}, \matr{q}$. We may refer to this basis as $\matr{e}_\mu$ and its 4 vectors as $\matr{e}_0, \matr{e}_1, \matr{e}_2, \matr{e}_3$. In this notation, we decided to put an index of each basis vector in the subscript. *That is a very important choice that will be extremely consequential later*.

A vector $\matr{A} \in \mathcal{V}$ can be expressed as $a\matr{e}_0 + b\matr{e}_1 + c\matr{e}_2 + d\matr{e}_3$. To avoid using new variable for each component (real number), we can introduce the following (note the absence of boldface):
\begin{align}
\begin{split}
  \matr{A} &= A^0 e_0 + A^1 e_1 + A^2 e_2 + A^3 e_3 \\
           &= A^\mu e_\mu \quad \text{this is referred to as \textbf{Einstein summation convention}}
\end{split}
\end{align}

Suppose we have real vectors spaces $\mathcal{V}$ and $\mathcal{W}$. $\dim{\mathcal{V}} = 4$ and $\dim{\mathcal{W}} = 10$, while the basis of $\mathcal{V}$ is $e_\mu$ and the basis of $\mathcal{W}$ is $f_\mu$. Can we make a map $\Lambda$ that maps any vector in $\mathcal{V}$ to a vector in $\mathcal{W}$?

Such a map would mathematically be written in the following way:
\begin{equation}
  \Lambda: \mathcal{V} \to \mathcal {W}
\end{equation}
we would say "$\Lambda$ maps from the domain $\mathcal{V}$ to the range $\mathcal{W}$".

We can then talk of $\Lambda$ acting on a specific vector $\matr{v}$ in $\mathcal{V}$ and giving us vector $\matr{w}$ in $\mathcal{W}$ in either of the following ways:
\begin{align}
\begin{split}
  \Lambda \matr{v} &\to \matr{w} \\
  &\text{or} \\
  \Lambda(\matr{v}) &\to \matr{w} \\
\end{split}
\end{align}

However, we will usually use the following ("bracket") notation:
\begin{equation}
  \inner{\Lambda}{\matr{v}} \to \matr{w}
\end{equation}

All we need to do is to define *linear* mappings for basis $e_\mu$:
\begin{align*}
\begin{split}
  \inner{\Lambda}{e_0} &= 3f_1 + 2f_4 + f_{10} \\
  \inner{\Lambda}{e_1} &= \pi f_3 + f_0 \\
  \inner{\Lambda}{e_2} &= f_2 \\
  \inner{\Lambda}{e_3} &= f_3 + f_5 +f_7 + f_9
\end{split}
\end{align*}

**Linear mappings** have two properties (suppose that $\matr{u}, \matr{v} \in \mathcal{V}$ and that $\Lambda: \mathcal{V} \to \mathcal{W}$):
\begin{align}
  \inner{\Lambda}{\matr{u} + \matr{v}} &= \inner{\Lambda}{\matr{u}} + \inner{\Lambda}{\matr{v}} &\text{(additivity)} \\
  \inner{\Lambda}{q \matr{u}} &= q \inner{\Lambda}{\matr{u}} \quad &\text{(homogeneity)}
\end{align}
all the terms on the RHS in the two above equations are vectors in $\mathcal{W}$.

Once we know the mappings for individual basis vectors of $\mathcal{V}$, we know the mappings for any vector in $\mathcal{V}$. That is because vector $\matr{A}$ in $\mathcal{V}$ is equal to $A^\mu e_\mu$ and so:
\begin{align}
\begin{split}
  \inner{\Lambda}{\matr{A}} &= \inner{\Lambda}{A^\mu e_\mu} \\
  &= A^\mu \inner{\Lambda}{e_\mu}
\end{split}
\end{align}

# Dual spaces

Suppose we have real vector space $\mathcal{V}$ with $\dim{\mathcal{V}} = 4$ and basis $e_\mu$. Suppose also that we have have a real vector space $\mathcal{W}$ with basis $f_\nu$ and which could have any number of dimensions.

Could we have maps from $\mathcal{V}$ without defining another vector space that we would be mapping to? If we do not have another vector space defined, we could only map the vector space $\mathcal{V}$ to itself or to $\mathbb{R}$. One can think of $\mathbb{R}$ as a vector space of dimension 1 and basis $e$ (not the Euler's number!). Mappings from $\mathcal{V}$ to $\mathbb{R}$ are important because we are not adding anything to our universe---$\mathbb{R}$ was used to define $\mathcal{V}$ in the first place.

But can we also avoid creating an arbitrary map from $\mathcal{V}$ to $\mathbb{R}$? Instead of considering *a* map, we could consider the set of *all* possible maps. That set of all possible maps is called the **dual space**. For vector space $\mathcal{V}$, its dual space is called $\mathcal{V}^*$ ("v-star").

We will now try to prove that dual space is a vector space. For this we need the following *definitions* (suppose that $\matr{v}$ is a vector in vector space $\mathcal{V}$, while $\Lambda$ and $\mathrm{Z}$ are maps in its dual space $\mathcal{V}^*$):
\begin{align}
  \inner{\Lambda + \mathrm{Z}}{\matr{v}} &= \inner{\Lambda}{\matr{v}} + \inner{\mathrm{Z}}{\matr{v}} \\
  \inner{a \Lambda}{\matr{v}} &= a \inner{\Lambda}{\matr{v}} 
\end{align}

If we also say that scalar multiplication by $-1$ gives an opposite map, we now have satisfied all the requirements for $\mathcal{V}^*$ to be a vector space. This shows that we cannot create a *single* vector space. Whenever we create $\mathcal{V}$, we automatically also create $\mathcal{V}^*$. We will use subscripts to denote the basis of "normal" vector space and a superscript to denote the basis of its dual space.

How do we choose the basis of the dual space though? It is arbitrary! However, we can define a convenient rule that the bases should have to satisfy:
\begin{equation}
  \inner{e^\mu}{e_\nu} = \delta_\nu^\mu
\end{equation}

# Cartesian products

Dual space, $\mathcal{V}^{**}$, of $\mathcal{V}^*$ is simply $\mathcal{V}$.

Arbitrary map in a vector space can be expressed as $A^\mu e_\mu$, while an arbitrary map in its dual space can be expressed as $\beta_\mu e^\mu$. The former can be called a vector, and the latter---a covector.

Suppose we have arbitrary mapping:
\begin{align}
\begin{split}
  \inner{\beta_\mu e^\mu}{A^\nu e_\nu} &= \\
  \beta_\mu A^\nu \inner{e^\mu}{e_\nu} &= \\
  \beta_\mu A^\nu \delta_\nu^\mu &= \beta_\mu A^\mu
\end{split}
\end{align}

Although this is similar to dot product, it is not a dot product. Traditionally, dot product is defined for two vectors from the *same* vector space, while here the vectors are from different vector spaces (vector space and its dual space).

**Cartesian product** is a binary operation that takes as an input two sets and  returns another set as its output. If we have two sets $A$ and $B$, their Cartesian product $A \times B$ returns all ordered pairs of the elements from the two sets:

\begin{equation}
  A \times B = \{a, b, c, \dots \} \times \{\alpha, \beta, \gamma, \dots \} = \{ (a, \alpha), (a, \beta), \dots, (c, \beta), (c, \gamma), \dots \}
\end{equation}

These sets do not necessarily have to be vectors spaces, but vector spaces are sets and thus we can perform this operation on them. Cartesian products allow to define arbitrary binary operations. That is because binary operation that takes as an input $A$ and $B$ and outputs $C$ can be thought of as a mapping from $A \times B$ to $C$, i.e. we have to define the operation for every possible pair of elements from $A$ and $B$.

# Tensor products

We want to create a map that maps from $\mathcal{V} \times \mathcal{V}$ to $\mathbb{R}$. We will call this mapping a **tensor product**.

Suppose that $v, w \in \mathcal{V}$ and $\alpha, \beta \in \mathcal{V}^*$. Then the tensor product, $\alpha \otimes \beta$, operating on the pair $(v, w)$ will be defined as:
\begin{equation} \label{eq:covector_tensor_product_on_vector_cartesian_product}
  \left[ \alpha \otimes \beta \right] (v, w) = \inner{\alpha}{v} \inner{\beta}{w}
\end{equation}

One could name $\alpha \otimes \beta$ with some symbol, say $\Lambda$, because it is a mapping. However, that way it is easy to forget that it acts on two vectors, not just one.

We can make all tensor products of two covectors linear by making the following *definition*: if $\alpha, \beta, \gamma, \delta \in \mathcal{V}^*$, $v, w \in \mathcal{V}$ and $a, b \in \mathbb{R}$, then
\begin{align}
\begin{split}
  \left[ a \left( \alpha \otimes \beta \right) \right] \left[ b \left( \gamma \otimes \delta \right) \right] (v, w) &= \\
  a \left[ \alpha \otimes \beta \right] (v, w) + b \left[ \gamma \otimes \delta \right] (v, w) &= a \inner{\alpha}{v} \inner{\beta}{w} + b \inner{\gamma}{v} \inner{\delta}{w} 
\end{split}
\end{align}

If $\mathcal{V}$ has bases $e_\mu$ and $\mathcal{V}^*$ has bases $e^\nu$, then $\mathcal{V}^* \otimes \mathcal{V}^*$ has bases $e^\nu \otimes e^\xi: \left\{ e^0 \otimes e^0, e^0 \otimes e^1, \dots e^3 \otimes e^3 \right\}$ (if $\dim \mathcal{V}^* = 4$).

Let us look at an example:
\begin{align*}
  \left[ e^1 \otimes e^2 \right] \left( A^\mu e_\mu, B^\nu e_\nu \right) &= \\
  \inner{e^1}{A^\mu e_\mu} \inner{e^2}{B^\nu e_\nu} &= \\
  A^\mu \inner{e^1}{e_\mu} B^\nu \inner{e^2}{e_\nu} &= \\
  A^\mu \delta_\mu^1 B^\nu \delta _\nu^2 = A^1 B^2
\end{align*}

If an arbitrary vector in $\mathcal{V}$ can be written as $A^\mu e_\mu$ and arbitrary covector in $\mathcal{V}^*$ can be written as $B_\nu e^\nu$, how could we write an arbitrary tensor product of covectors? Now we have 16 bases and it might make sense to simply use two indices:
\begin{align}
\begin{split}
  \alpha \otimes \gamma &\equiv \tensor{T}{_{\mu \nu}} e^\mu \otimes e^\nu \\
  &= \tensor{T}{_{00}} e^0 \otimes e^0 + \tensor{T}{_{01}} e^0 \otimes e^1 + \dots + \tensor{T}{_{33}} e^3 \otimes e^3
\end{split}
\end{align}

# Tensor product spaces

We saw that tensor product of dual spaces can act as map from Cartesian product of vector spaces to real numbers, i.e.:
\begin{equation}
  \mathcal{V}^* \otimes \mathcal{V}^*: \mathcal{V} \times \mathcal{V} \to \mathbb{R}
\end{equation}
with an arbitrary member of the tensor product space $\mathcal{V}^* \otimes \mathcal{V}^*$ given by:
\begin{equation}
  \tensor{T}{_{\alpha \beta}} e^\alpha \otimes e^\beta
\end{equation}

We can actually have Cartesian products of more than two vector spaces, and accordingly mapping to real numbers consist of tensor product of dual spaces:
\begin{equation}
  \mathcal{V}^* \otimes \mathcal{V}^* \otimes \mathcal{V}^* \otimes \dots: \mathcal{V} \times \mathcal{V} \times \mathcal{V} \times \dots \to \mathbb{R}
\end{equation}
with an arbitrary member of the tensor product space $\mathcal{V}^* \otimes \mathcal{V}^* \otimes \mathcal{V}^* \otimes \dots$ given by:
\begin{equation}
  \tensor{T}{_{\alpha \beta \gamma \dots}} e^\alpha \otimes e^\beta \otimes e^\gamma \otimes \dots
\end{equation}

Similarly, we can have tensor products of vector spaces acting as maps from the Cartesian products of dual spaces to real numbers:
\begin{equation}
  \mathcal{V} \otimes \mathcal{V} \otimes \mathcal{V} \otimes \dots: \mathcal{V}^* \times \mathcal{V}^* \times \mathcal{V}^* \times \dots \to \mathbb{R}
\end{equation}
with an arbitrary member of the tensor product space $\mathcal{V} \otimes \mathcal{V} \otimes \mathcal{V} \otimes \dots$ given by:
\begin{equation}
  \tensor{T}{^{\alpha \beta \gamma \dots}} e_\alpha \otimes e_\beta \otimes e_\gamma \otimes \dots
\end{equation}

Quickly, we should understand how tensor product of vector spaces acts on Cartesian products of dual spaces. Suppose that $\alpha, \beta \in \mathcal{V}^*$ and $v, w \in \mathcal{V}$. Then the tensor product, $v \otimes w$, operating on the pair $(\alpha, \beta)$ will be defined as:
\begin{equation}
  \left[ w \otimes w \right] (\alpha, \beta) = \inner{\alpha}{v} \inner{\beta}{w}
\end{equation}
Note that the order is the same as in equation \ref{eq:covector_tensor_product_on_vector_cartesian_product}.

Furthermore, we can have tensor products of both vector spaces and dual spaces which could act as maps from Cartesian products of both dual spaces and vector spaces to real numbers. The order in which the vector spaces and dual spaces are arranged is important because members of Cartesian product spaces are *ordered* pairs (or lists, if more than two). For example, we could have:
\begin{equation}
  \mathcal{V} \otimes \mathcal{V}^* \otimes \mathcal{V}^* \otimes \mathcal{V} \otimes \mathcal{V}^*: \mathcal{V}^* \times \mathcal{V} \times \mathcal{V} \times \mathcal{V}^* \times \mathcal{V} \to \mathbb{R}
\end{equation}
with an arbitrary member of the tensor product space $\mathcal{V} \otimes \mathcal{V}^* \otimes \mathcal{V}^* \otimes \mathcal{V} \otimes \mathcal{V}^*$ given by:
\begin{equation}
  \tensor{T}{^\alpha _{\beta \gamma} ^\delta _\epsilon} e_\alpha \otimes e^\beta \otimes e^\gamma \otimes e_\delta \otimes e^\epsilon
\end{equation}

Note the whitespace in component $\tensor{T}{^\alpha _{\beta \gamma} ^\delta _\epsilon}$---it is important to retain the information about the order of vector spaces and dual spaces because bases are often not provided, only the component is given.

# Rank of a tensor product space

We usually say that tensors are tensor products of vector spaces followed by dual spaces. For example, $\mathcal{V} \otimes \mathcal{V}^* \otimes \mathcal{V}^*$ would be called a tensor while $\mathcal{V}^* \otimes \mathcal{V} \otimes \mathcal{V}$ would not (even though both are multilinear maps).

Rank (defined only for tensors, as described above) of a tensor product space is a pair of numbers: the first one is the number of vector spaces and the second one is the number of dual spaces in the tensor product. A vector space would have rank $(1, 0)$, a dual space would have rank $(0, 1)$, $\mathcal{V} \otimes \mathcal{V}^* \otimes \mathcal{V}^*$ would have rank $(1, 2)$, etc. We will refer to tensors of rank $(a, b)$ as $\mathcal{T}^a_b$.

Suppose that we have a tensor product space element $\tensor{T}{^\mu_{\nu \lambda}} e_\mu \otimes e^\nu \otimes e^\lambda$ operating on $\left( A_\xi e^\xi, B^\delta e_\delta, C^\gamma e_\gamma \right)$:
\begin{align}
\begin{split}
  \tensor{T}{^\mu_{\nu \lambda}} \left[ e_\mu \otimes e^\nu \otimes e^\lambda \right] \left( A_\xi e^\xi, B^\delta e_\delta, C^\gamma e_\gamma \right)  &= \tensor{T}{^\mu_{\nu \lambda}} \inner{A_\xi e^\xi}{e_\mu} \inner{e^\nu}{B^\delta e_\delta} \inner{e^\lambda}{C^\gamma e_\gamma} \\
  &= \tensor{T}{^\mu_{\nu \lambda}} A_\xi B^\delta C^\gamma \inner{e^\xi}{e_\mu} \inner{e^\nu}{e_\delta} \inner{e^\lambda}{e_\gamma} \\
  &= \tensor{T}{^\mu_{\nu \lambda}} A_\xi B^\delta C^\gamma \delta^\xi_\mu \delta^\nu_\delta \delta^\lambda_\gamma \\
  &= \tensor{T}{^\mu_{\nu \lambda}} A_\mu B^\nu C^\lambda
\end{split}
\end{align}
Usually, one would simply use the expression in the last line because this procedure applies to all tensor product spaces.

# Linear transformations

Suppose that we have vector space $\mathcal{V}$ and a vector $w$ in it. This vector could be expressed using a set of bases $e_\mu$ as $A^\mu e_\mu$, for example. Equivalently, it could be expressed using a set of bases $f_\nu$ as $B^\nu f_\nu$, for example. That is:
\begin{equation}
  w = A^\mu e_\mu = B^\nu f_\nu
\end{equation}

Any base $f_\nu$ can be expressed as a linear combinations of bases $e_\mu$. That is because bases are vectors too. For each of the bases, we would have:
\begin{align}
\begin{split}
  f_0 &= \tensor{\Lambda}{_0^0} e_0 + \tensor{\Lambda}{_0^1} e_1 + \tensor{\Lambda}{_0^2} e_2 + \tensor{\Lambda}{_0^3} e_3 = \tensor{\Lambda}{_0^\mu} e_\mu \\
  f_1 &= \tensor{\Lambda}{_1^0} e_0 + \tensor{\Lambda}{_1^1} e_1 + \tensor{\Lambda}{_1^2} e_2 + \tensor{\Lambda}{_1^3} e_3 = \tensor{\Lambda}{_1^\mu} e_\mu \\
  f_2 &= \tensor{\Lambda}{_2^0} e_0 + \tensor{\Lambda}{_2^1} e_1 + \tensor{\Lambda}{_2^2} e_2 + \tensor{\Lambda}{_2^3} e_3 = \tensor{\Lambda}{_2^\mu} e_\mu \\
  f_3 &= \tensor{\Lambda}{_3^0} e_0 + \tensor{\Lambda}{_3^1} e_1 + \tensor{\Lambda}{_3^2} e_2 + \tensor{\Lambda}{_3^3} e_3 = \tensor{\Lambda}{_3^\mu} e_\mu 
\end{split}
\end{align}
Or more compactly:
\begin{equation} \label{eq:f_to_e_base_change}
  f_\nu  = \tensor{\Lambda}{_\nu^\mu} e_\mu 
\end{equation}
here $f_\nu$ and $e_\mu$ are rank-1 tensors, while $\tensor{\Lambda}{_\nu^\mu}$ is a matrix (not a tensor!).

So if we have a vector expressed as $w = B^\nu e_\nu$, we can use equation \ref{eq:f_to_e_base_change} to use base $e$ instead:
\begin{equation}
  w = B^\nu e_\nu = B^\nu \tensor{\Lambda}{_\nu^\mu} e_\mu
\end{equation}
here $B^\nu \tensor{\Lambda}{_\nu^\mu}$ are just real numbers (coefficients). We can thus say that:
\begin{equation}
  A^\mu = B^\nu \tensor{\Lambda}{_\nu^\mu}
\end{equation}
this is a transformation that allows to go from coefficients in $e$ basis to coefficients in $f$ basis.

How do we go from $e$ basis to $f$ basis and from coefficients in $f$ basis to coefficients in $e$ basis? For that we would need to find $e$ in terms of $f$:\footnote{not completely sure about derivation but the answer should be correct}
\begin{align} \label{eq:e_to_f_base_change} 
\begin{split}
  f_\nu &= \tensor{\Lambda}{_\nu^\mu} e_\mu \\
  \tensor{\left( \Lambda^{-1} \right)}{_\gamma^\nu} f_\nu &= \tensor{\left( \Lambda^{-1} \right)}{_\gamma^\nu} \tensor{\Lambda}{_\nu^\mu} e_\mu \\
  \tensor{\left( \Lambda^{-1} \right)}{_\gamma^\nu} f_\nu &= \delta_\gamma^\mu e_\mu \\
  \tensor{\left( \Lambda^{-1} \right)}{_\gamma^\nu} f_\nu &= e_\gamma \\
   e_\gamma &= \tensor{\left( \Lambda^{-1} \right)}{_\gamma^\nu} f_\nu \quad \text{switch sides of the equation} \\
   e_\mu &= \tensor{\left( \Lambda^{-1} \right)}{_\mu^\nu} f_\nu \quad \text{rename $\gamma$ to $\mu$} \
\end{split}
\end{align}

So if we have a vector expressed as $w = A^\mu e_\mu$, we can use equation \ref{eq:e_to_f_base_change} to use base $f$ instead:
\begin{equation}
  w = A^\mu e_\mu = A^\mu \tensor{\left( \Lambda^{-1} \right)}{_\mu^\nu} f_\nu 
\end{equation}
here $A^\mu \tensor{\left( \Lambda^{-1} \right)}{_\mu^\nu}$ are just real numbers (coefficients). We can thus say that:
\begin{equation}
  B^\nu = A^\mu \tensor{\left( \Lambda^{-1} \right)}{_\mu^\nu} 
\end{equation}
this is a transformation that allows to go from coefficients in $f$ basis to coefficients in $e$ basis.

We say that bases $f$ transform **covariantly** (because they use matrix $\Lambda$), while coefficients $B$ transform **contravariantly** (because they use the inverse of matrix $\Lambda$).

We can additionally make sure that all these transformations make sense:
\begin{align}
\begin{split}
   w &= B^\nu f_\nu \\
     &= \underbrace{A^\epsilon \tensor{\left( \Lambda^{-1} \right)}{_\epsilon^\nu}}_{B^\nu} \underbrace{\tensor{\Lambda}{_\nu^\gamma} e_\gamma}_{f_\nu} \\
     &= A^\epsilon \delta_\epsilon^\gamma e_\gamma \\
     &= A^\mu e_\mu
\end{split}
\end{align}
