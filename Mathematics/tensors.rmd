---
title: "Tensors"
---

# What is a Tensor?

## Elementary vector spaces

Vector space is a set of vectors. However, it is important to talk about specific requirements that this mathematical set has to satisfy. 

Properties of vector space $\mathcal{W}$:

1. **Vector addition**. One should be able to add vectors, but only those that are in the vector space. If $\matr{w}, \matr{s} \in \mathcal{W}$, then $\matr{w} + \matr{s} = \matr{t} \in \mathcal{W}$.
2. **Scalar multiplication**. If $a \in \mathbb{R}$ and $\matr{w} \in \mathcal{W}$, then $a\matr{w} \in \mathcal{W}$. Vector spaces can be either real or complex, depending on what type of vector $a$ is.
3. **Linearity**:
   * $a\matr{w} + b\matr{v} = \matr{z}$
   * $a\matr{w} + a\matr{t} = a(\matr{w} + \matr{t})$
4. **Additive inverses**. If $\matr{w} \in \mathcal{W}$, then there exists another vector $-\matr{w} \in \mathcal{W}$, so that $\matr{w} + (-\matr{w}) = \matr{0}$.

Suppose we have $\matr{w}, \matr{q} \in \mathcal{V}$ and $\matr{r}, \matr{s} \in \mathcal{W}$. We can add $\matr{w}$ and $\matr{q}$ together or $\matr{r}$ and $\matr{s}$ together but not $\matr{w}$ and $\matr{r}$ together. Addition represents different operations for different vector spaces. Or at least there is no definition of how to add vectors that are not part of a particular vector space.

It is often not possible to distinguish between two vector spaces. But one property that often helps to distinguish them is their dimension. If $n$ is the *minimum* number of vectors that can be used to distinguish *any* vector in vector space $\mathcal{V}$ as their linear combination, then the dimension of $\mathcal{V}$ is $n$, i.e. $\dim{\mathcal{V}} = n$. These $n$ vectors are usually referred to as **basis vectors**. Basis vectors are not unique because if $\matr{v}$ is a basis vector, then so is $a\matr{v}$.

If two vector spaces $\mathcal{V}$ and $\mathcal{W}$ are real vector spaces and both have dimension $n$, then the difference between them is superficial---only their names are different. We call such vector spaces **isomorphic**. More precisely, two vector spaces are isomorphic if there is a one-to-one correspondence between them and if operations in one vector space are in correspondence to operations in another vector space.

In elementary vector spaces, concepts, such as dot product of vectors, cross product of vectors or magnitude of vectors, do not exist. These properties are advanced and have to be added to elementary vector spaces.

## How to make a map

Suppose we have a real vector space $\mathcal{V}$ with $\dim{\mathcal{V}} = 4$. Suppose also that we can have basis vectors $\matr{w}, \matr{v}, \matr{p}, \matr{q}$. We may refer to this basis as $\matr{e}_\mu$ and its 4 vectors as $\matr{e}_0, \matr{e}_1, \matr{e}_2, \matr{e}_3$. In this notation, we decided to put an index of each basis vector in the subscript. *That is a very important choice that will be extremely consequential later*.

A vector $\matr{A} \in \mathcal{V}$ can be expressed as $a\matr{e}_0 + b\matr{e}_1 + c\matr{e}_2 + d\matr{e}_3$. To avoid using new variable for each component (real number), we can introduce the following (note the absence of boldface):
\begin{align}
\begin{split}
  \matr{A} &= A^0 e_0 + A^1 e_1 + A^2 e_2 + A^3 e_3 \\
           &= A^\mu e_\mu \quad \text{this is referred to as \textbf{Einstein summation convention}}
\end{split}
\end{align}

Suppose we have real vectors spaces $\mathcal{V}$ and $\mathcal{W}$. $\dim{\mathcal{V}} = 4$ and $\dim{\mathcal{W}} = 10$, while the basis of $\mathcal{V}$ is $e_\mu$ and the basis of $\mathcal{W}$ is $f_\mu$. Can we make a map $\Lambda$ that maps any vector in $\mathcal{V}$ to a vector in $\mathcal{W}$?

Such a map would mathematically be written in the following way:
\begin{equation}
  \Lambda: \mathcal{V} \to \mathcal {W}
\end{equation}
we would say "$\Lambda$ maps from the domain $\mathcal{V}$ to the range $\mathcal{W}$".

We can then talk of $\Lambda$ acting on a specific vector $\matr{v}$ in $\mathcal{V}$ and giving us vector $\matr{w}$ in $\mathcal{W}$ in either of the following ways:
\begin{align}
\begin{split}
  \Lambda \matr{v} &\to \matr{w} \\
  &\text{or} \\
  \Lambda(\matr{v}) &\to \matr{w} \\
\end{split}
\end{align}

However, we will usually use the following ("bracket") notation:
\begin{equation}
  \inner{\Lambda}{\matr{v}} \to \matr{w}
\end{equation}

All we need to do is to define *linear* mappings for basis $e_\mu$:
\begin{align*}
\begin{split}
  \inner{\Lambda}{e_0} &= 3f_1 + 2f_4 + f_{10} \\
  \inner{\Lambda}{e_1} &= \pi f_3 + f_0 \\
  \inner{\Lambda}{e_2} &= f_2 \\
  \inner{\Lambda}{e_3} &= f_3 + f_5 +f_7 + f_9
\end{split}
\end{align*}

**Linear mappings** have two properties (suppose that $\matr{u}, \matr{v} \in \mathcal{V}$ and that $\Lambda: \mathcal{V} \to \mathcal{W}$):
\begin{align}
  \inner{\Lambda}{\matr{u} + \matr{v}} &= \inner{\Lambda}{\matr{u}} + \inner{\Lambda}{\matr{v}} &\text{(additivity)} \\
  \inner{\Lambda}{q \matr{u}} &= q \inner{\Lambda}{\matr{u}} \quad &\text{(homogeneity)}
\end{align}
all the terms on the RHS in the two above equations are vectors in $\mathcal{W}$.

Once we know the mappings for individual basis vectors of $\mathcal{V}$, we know the mappings for any vector in $\mathcal{V}$. That is because vector $\matr{A}$ in $\mathcal{V}$ is equal to $A^\mu e_\mu$ and so:
\begin{align}
\begin{split}
  \inner{\Lambda}{\matr{A}} &= \inner{\Lambda}{A^\mu e_\mu} \\
  &= A^\mu \inner{\Lambda}{e_\mu}
\end{split}
\end{align}

## Dual spaces

Suppose we have real vector space $\mathcal{V}$ with $\dim{\mathcal{V}} = 4$ and basis $e_\mu$. Suppose also that we have have a real vector space $\mathcal{W}$ with basis $f_\nu$ and which could have any number of dimensions.

Could we have maps from $\mathcal{V}$ without defining another vector space that we would be mapping to? If we do not have another vector space defined, we could only map the vector space $\mathcal{V}$ to itself or to $\mathbb{R}$. One can think of $\mathbb{R}$ as a vector space of dimension 1 and basis $e$ (not the Euler's number!). Mappings from $\mathcal{V}$ to $\mathbb{R}$ are important because we are not adding anything to our universe---$\mathbb{R}$ was used to define $\mathcal{V}$ in the first place.

But can we also avoid creating an arbitrary map from $\mathcal{V}$ to $\mathbb{R}$? Instead of considering *a* map, we could consider the set of *all* possible maps. That set of all possible maps is called the **dual space**. For vector space $\mathcal{V}$, its dual space is called $\mathcal{V}^*$ ("v-star").

We will now try to prove that dual space is a vector space. For this we need the following *definitions* (suppose that $\matr{v}$ is a vector in vector space $\mathcal{V}$, while $\Lambda$ and $\mathrm{Z}$ are maps in its dual space $\mathcal{V}^*$):
\begin{align}
  \inner{\Lambda + \mathrm{Z}}{\matr{v}} &= \inner{\Lambda}{\matr{v}} + \inner{\mathrm{Z}}{\matr{v}} \\
  \inner{a \Lambda}{\matr{v}} &= a \inner{\Lambda}{\matr{v}} 
\end{align}

If we also say that scalar multiplication by $-1$ gives an opposite map, we now have satisfied all the requirements for $\mathcal{V}^*$ to be a vector space. This shows that we cannot create a *single* vector space. Whenever we create $\mathcal{V}$, we automatically also create $\mathcal{V}^*$. We will use subscripts to denote the basis of "normal" vector space and a superscript to denote the basis of its dual space.

How do we choose the basis of the dual space though? It is arbitrary! However, we can define a convenient rule that the bases should have to satisfy:
\begin{equation}
  \inner{e^\mu}{e_\nu} = \delta_\nu^\mu
\end{equation}

# Cartesian products

Dual space, $\mathcal{V}^{**}$, of $\mathcal{V}^*$ is simply $\mathcal{V}$.

Arbitrary map in a vector space can be expressed as $A^\mu e_\mu$, while an arbitrary map in its dual space can be expressed as $\beta_\mu e^\mu$. The former can be called a vector, and the latter---a covector.

Suppose we have arbitrary mapping:
\begin{align}
  \inner{\beta_\mu e^\mu}{A^\nu e_\nu} &= \\
  \beta_\mu A^\nu \inner{e^\mu}{e_\nu} &= \\
  \beta_\mu A^\nu \delta_\nu^\mu &= \beta_\mu A^\mu
\end{align}

Although this is similar to dot product, it is not a dot product. Traditionally, dot product is defined for two vectors from the *same* vector space, while here the vectors are from different vector spaces (vector space and its dual space).

**Cartesian product** is a binary operation that takes as an input two sets and  returns another set as its output. If we have two sets $A$ and $B$, their Cartesian product $A \times B$ returns all ordered pairs of the elements from the two sets:

\begin{equation}
  A \times B = \{a, b, c, \dots \} \times \{\alpha, \beta, \gamma, \dots \} = \{ (a, \alpha), (a, \beta), \dots, (c, \beta), (c, \gamma), \dots \}
\end{equation}

These sets do not necessarily have to be vectors spaces, but vector spaces are sets and thus we can perform this operation on them. Cartesian products allow to define arbitrary binary operations. That is because binary operation that takes as an input $A$ and $B$ and outputs $C$ can be thought of as a mapping from $A \times B$ to $C$, i.e. we have to define the operation for every possible pair of elements from $A$ and $B$.

# Geometrical Methods of Mathematical Physics

## Space $\mathbb{R}^n$ and its topology

**Distance function** between two points $\matr{x} = (x_1, \dots, x_n)$ and $\matr{y} = (y_1, \dots, y_n)$ is:
\begin{equation}
  d(\matr{x}, \matr{y}) = \left[ \sum\limits_{i=1}^n (x_i - y_i)^2 \right]^{1/2}
\end{equation}

**Neighbourhood** of radius $r$ of the point $\matr{x}$ in $\mathbb{R}^n$ is the set of points $N_r(\matr{x})$ whose distance from $\matr{x}$ is *less* than $r$.

A set of points $\matr{S}$ is **open** if every point $\matr{x}$ in $\matr{S}$ has a neighbourhood entirely contained in $\matr{S}$. 

**Open ball** of radius $r$ centred at $\matr{x}$ is the collection of all the points of distance less than $r$ from $\matr{x}$, i.e.:
\begin{equation}
  B_r(\matr{x}) = \left\{\matr{y}: d(\matr{x}, \matr{y}) < r \right\}
\end{equation}

**Closed ball** of radius $r$ centred at $\matr{x}$ is the collection of all the points of distance less than or equal to $r$ from $\matr{x}$, i.e.:
\begin{equation}
  B_r(\matr{x}) = \left\{\matr{y}: d(\matr{x}, \matr{y}) \leq r \right\}
\end{equation}

**Hausdorff space** is a topological space where for any two points there exist neighbourhoods of each which are disjoint from each other.

For spaces, such as $\mathbb{R}^n$, the exact definition of distance function is not important when discussing its topology. For example, we can use many different definitions of a distance function to define neighbourhoods. We only need the notion that the distance between two distinct points can be made arbitrarily small but that it would not become zero. When topology of a space is not strictly dependent on definition of distance function, we usually call it **natural topology**.

### Proof: open ball is an open set

Suppose that $B_r(\matr{x})$ is an open ball.

For any $\matr{y} \in B_r(\matr{x})$, let $r_1 = r - d(\matr{x}, \matr{y})$.

\begin{equation} \label{eq:ball_radius}
\Rightarrow r_1 + d(\matr{x}, \matr{y}) = r
\end{equation}

Suppose that $\matr{z} \in B_{r_1}(\matr{y})$. Then by triangle inequality:

\begin{equation} \label{eq:triangle_inequality}
  d(\matr{x}, \matr{z}) \leq d(\matr{x}, \matr{y}) + d(\matr{y}, \matr{z})
\end{equation}

Because $\matr{z} \in B_{r_1}(\matr{y})$:

\begin{equation}
  d(\matr{y}, \matr{z}) < r_1
\end{equation}

Then:

\begin{equation}
  d(\matr{y}, \matr{z}) + d(\matr{y}, \matr{z}) < r_1 + d(\matr{y}, \matr{z}) 
\end{equation}

Using equation \ref{eq:ball_radius}: 
\begin{equation}
  d(\matr{y}, \matr{z}) + d(\matr{y}, \matr{z}) < r
\end{equation}

Combining with equation \ref{eq:triangle_inequality}:
\begin{equation}
  d(\matr{x}, \matr{z}) \leq d(\matr{x}, \matr{y}) + d(\matr{y}, \matr{z}) < r
\end{equation}

Thus:
\begin{equation}
  d(\matr{x}, \matr{z}) < r
\end{equation}

Thus $B_r(\matr{x})$ is an open set. 
